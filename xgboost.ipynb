{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset_lvl3_5.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 20\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#Last three overs\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# df = pd.read_pickle('dataset_lvl3_3.pkl')\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Mean Absolute Error: 3.3719844032441086\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Last five overs\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdataset_lvl3_5.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Mean Absolute Error: 3.018922742290367\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;66;03m# MSE: 34.812845168106065\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# RMSE: 5.900241111014537\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;66;03m# RMSE: 6.031975229911066\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;66;03m# r squared: 0.9678022708130165\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/pickle.py:189\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;03mLoad pickled pandas object (or any object) from file.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;124;03m4    4    9\u001b[39;00m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    188\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 189\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.11/site-packages/pandas/io/common.py:872\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    868\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    873\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    875\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset_lvl3_5.pkl'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, classification_report, confusion_matrix, roc_auc_score,roc_curve, auc, precision_recall_curve\n",
    "import pandas as pd\n",
    "\n",
    "#Last three overs\n",
    "# df = pd.read_pickle('dataset_lvl3_3.pkl')\n",
    "    # Mean Absolute Error: 3.3719844032441086\n",
    "    # MSE: 46.381124990387306\n",
    "    # RMSE: 6.810368932032046\n",
    "    # r squared: 0.9589562132275067\n",
    "\n",
    "# Last four overs\n",
    "# df = pd.read_pickle('dataset_lvl3_4.pkl')\n",
    "    # Mean Absolute Error: 3.0909708904172466\n",
    "    # MSE: 34.917820941126244\n",
    "    # RMSE: 5.909130303278668\n",
    "    # r squared: 0.9691003700844963\n",
    "\n",
    "# Last five overs\n",
    "df = pd.read_pickle('dataset_lvl3.pkl')\n",
    "    # Mean Absolute Error: 3.018922742290367\n",
    "    # MSE: 34.812845168106065\n",
    "    # RMSE: 5.900241111014537\n",
    "    # r squared: 0.9691932657019486\n",
    "\n",
    "# Last 6 overs\n",
    "# df = pd.read_pickle('dataset_lvl3_6.pkl')\n",
    "    # Mean Absolute Error: 3.0687805082394335\n",
    "    # MSE: 35.09432002639376\n",
    "    # RMSE: 5.924045916972096\n",
    "    # r squared: 0.9689441817466161\n",
    "\n",
    "# Last 7 overs\n",
    "# df = pd.read_pickle('dataset_lvl3_7.pkl')\n",
    "    # Mean Absolute Error: 2.977061690177932\n",
    "    # MSE: 36.384725174260666\n",
    "    # RMSE: 6.031975229911066\n",
    "    # r squared: 0.9678022708130165\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_columns = ['batting_team', 'bowling_team', 'current_score', 'wickets_remaining', 'city', 'balls_left', 'last_five']\n",
    "target_columns = ['final_score']\n",
    "\n",
    "X = df[data_columns]\n",
    "y = df[target_columns]\n",
    "\n",
    "X_encoded = pd.get_dummies(X, columns=['batting_team', 'bowling_team', 'city'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.018922742290367\n",
      "MSE: 34.812845168106065\n",
      "RMSE: 5.900241111014537\n",
      "r squared: 0.9691932657019486\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=1200,subsample=1.0, learning_rate=0.46, max_depth=11,random_state=42)\n",
    "\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = mse ** 0.5\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Absolute Error: {mae}\")\n",
    "print(f\"MSE: {mse}\")\n",
    "print(f\"RMSE: {rmse}\")\n",
    "print(f\"r squared: {r2}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'learning_rate': 0.46, 'max_depth': 11, 'n_estimators': 1600, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [11],\n",
    "    'learning_rate': [0.46],\n",
    "    'n_estimators': [1200, 1400, 1600],\n",
    "    'subsample': [1.0]\n",
    "}\n",
    "\n",
    "xgb_reg = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=xgb_reg, param_grid=param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "best_model = grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batting_team</th>\n",
       "      <th>bowling_team</th>\n",
       "      <th>current_score</th>\n",
       "      <th>wickets_remaining</th>\n",
       "      <th>city</th>\n",
       "      <th>balls_left</th>\n",
       "      <th>last_five</th>\n",
       "      <th>final_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>India</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>Lauderhill</td>\n",
       "      <td>76</td>\n",
       "      <td>28.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>India</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>Lauderhill</td>\n",
       "      <td>75</td>\n",
       "      <td>28.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>India</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>Lauderhill</td>\n",
       "      <td>74</td>\n",
       "      <td>28.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>India</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>Lauderhill</td>\n",
       "      <td>73</td>\n",
       "      <td>29.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>West Indies</td>\n",
       "      <td>India</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>Lauderhill</td>\n",
       "      <td>72</td>\n",
       "      <td>30.0</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    batting_team bowling_team  current_score  wickets_remaining        city  \\\n",
       "155  West Indies        India             35                  5  Lauderhill   \n",
       "156  West Indies        India             36                  5  Lauderhill   \n",
       "157  West Indies        India             37                  5  Lauderhill   \n",
       "158  West Indies        India             37                  5  Lauderhill   \n",
       "159  West Indies        India             37                  5  Lauderhill   \n",
       "\n",
       "     balls_left  last_five  final_score  \n",
       "155          76       28.0           95  \n",
       "156          75       28.0           95  \n",
       "157          74       28.0           95  \n",
       "158          73       29.0           95  \n",
       "159          72       30.0           95  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[155:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted final scores: [104.65185]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n",
      "/var/folders/mn/d5k45yfx18jcxktw91qx9gbh0000gn/T/ipykernel_75233/3591763832.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_new_encoded[c] = 0\n"
     ]
    }
   ],
   "source": [
    "# test specific cases:\n",
    "new_data = [\n",
    "    {\n",
    "        'batting_team': 'West Indies', \n",
    "        'bowling_team': 'India', \n",
    "        'current_score': 4, \n",
    "        'wickets_remaining': 5, \n",
    "        'city': 'Lauderhill', \n",
    "        'balls_left': 76, \n",
    "        'last_five': 28\n",
    "    },\n",
    "    # Add more dictionaries for additional test cases\n",
    "]\n",
    "\n",
    "df_new = pd.DataFrame(new_data)\n",
    "\n",
    "df_new_encoded = pd.get_dummies(df_new, columns=['batting_team', 'bowling_team', 'city'])\n",
    "\n",
    "# Make sure all columns in X_test are present in df_new_encoded, filling missing ones with 0\n",
    "missing_cols = set(X_train.columns) - set(df_new_encoded.columns)\n",
    "for c in missing_cols:\n",
    "    df_new_encoded[c] = 0\n",
    "df_new_encoded = df_new_encoded[X_train.columns]\n",
    "\n",
    "# Load your trained model\n",
    "# Assuming xgb_model is your trained XGBoost model\n",
    "\n",
    "# Predict the final score\n",
    "final_score_predictions = xgb_model.predict(df_new_encoded)\n",
    "\n",
    "# Print the predictions\n",
    "print(\"Predicted final scores:\", final_score_predictions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgb_model.joblib']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(xgb_model, 'xgb_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data_set_lvl3_5.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m data \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata_set_lvl3_5.pkl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      6\u001b[0m data\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata_set_lvl3_5.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.11/lib/python/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data_set_lvl3_5.pkl'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "data = pickle.load(open(\"dataset_lvl3_5.pkl.pkl\", \"rb\"))\n",
    "\n",
    "data.to_csv('data_set_lvl3_5.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
